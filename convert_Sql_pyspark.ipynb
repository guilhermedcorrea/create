{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 sql            tipo  \\\n",
      "0                      select nome, peso from pessoa       firstname   \n",
      "1  SELECT Name, ProductNumber, ListPrice AS Price...  not_identified   \n",
      "2  SELECT p.Name AS ProductName, \\nNonDiscountSal...  not_identified   \n",
      "\n",
      "                                       pyspark_query  \n",
      "0                              df.select(nome, peso)  \n",
      "1  df.select(Name, ProductNumber, \"ListPrice\", \"P...  \n",
      "2  df.select(p.\"Name\", \"ProductName\", \\nNonDiscou...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import unidecode\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from typing import Dict, Any,List\n",
    "import warnings\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "warnings.filterwarnings(\"once\", message=\"This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers.*\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel\")\n",
    "\n",
    "#  palavras-chave SQL\n",
    "\n",
    "# Carrega as palavras-chave para SQL\n",
    "# Mapeamento de consulta SQL para palavras-chave\n",
    "# Mapeamento de consulta SQL para tipos PySpark\n",
    "\n",
    "sql_keywords: List[str] = [\n",
    "    'SELECT',\n",
    "    'FROM',\n",
    "    'WHERE',\n",
    "    'GROUP BY',\n",
    "    'HAVING',\n",
    "    \n",
    "]\n",
    "\n",
    "sql_to_pyspark_mapping: Dict[str, str] = {\n",
    "    'select': 'df.select',\n",
    "    'update': 'df.withColumn',\n",
    "    'merge': 'deltaTablePeople.alias(\\'people\\').merge(dfUpdates.alias(\\'updates\\'), \"people.id = updates.id\")',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "keywords_mapping: Dict[str, Dict[str, str]] = {\n",
    "    'select': {\n",
    "        'nome': 'firstname',\n",
    "        'idade': 'lastname',\n",
    "        'produto': 'product',\n",
    "        'preco': 'price',\n",
    "        \n",
    "    },\n",
    "    'update': {\n",
    "  \n",
    "    },\n",
    "    'merge': {\n",
    "        \n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in STOP_WORDS]\n",
    "    processed_text = \" \".join(filtered_tokens)\n",
    "    return processed_text\n",
    "\n",
    "def create_embeddings(texts: List[str]) -> np.ndarray:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "        \n",
    "        embeddings.append(output.last_hidden_state.mean(dim=1).numpy().flatten()) \n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "def classify_product(query_text: str, keywords_mapping: Dict[str, Dict[str, str]]) -> str:\n",
    "    query_text = preprocess_text(query_text)\n",
    "    for query_type, keywords in keywords_mapping.items():\n",
    "        for keyword, category in keywords.items():\n",
    "            if keyword in query_text:\n",
    "                return category\n",
    "    return 'not_identified'\n",
    "\n",
    "def convert_sql_to_pyspark(sql_query: str) -> str:\n",
    "\n",
    "    match = re.match(r'^\\s*(\\w+)\\s+', sql_query)\n",
    "    if match:\n",
    "        sql_type = match.group(1).lower()\n",
    "        if sql_type in sql_to_pyspark_mapping:\n",
    "            pyspark_query = sql_to_pyspark_mapping[sql_type]\n",
    "            \n",
    "          \n",
    "            fields_match = re.search(r'\\bselect\\b(.*?)\\bfrom\\b', sql_query, re.DOTALL | re.IGNORECASE)\n",
    "            if fields_match:\n",
    "                fields = fields_match.group(1).strip()\n",
    "                fields = re.sub(r'(\\w+)\\s+AS\\s+(\\w+)', r'\"\\1\", \"\\2\"', fields)\n",
    "                pyspark_query += f\"({fields})\"\n",
    "            \n",
    "            return pyspark_query\n",
    "    return 'not_identified'\n",
    "\n",
    "def main() -> None:\n",
    "    treino = pd.read_csv(r\"C:\\mlflowjobs\\treino.csv\", sep=\";\")\n",
    "    avaliacao = pd.read_csv(r\"C:\\mlflowjobs\\avaliacao.csv\", sep=\";\")\n",
    "    \n",
    "    treino['titulo_processed'] = treino['querysql'].apply(preprocess_text)\n",
    "    avaliacao['titulo_processed'] = avaliacao['sql'].apply(preprocess_text)\n",
    "    \n",
    "    treino_embeddings = create_embeddings(treino['titulo_processed'])\n",
    "    avaliacao_embeddings = create_embeddings(avaliacao['titulo_processed'])\n",
    "    \n",
    "    avaliacao['tipo'] = ''\n",
    "    avaliacao['pyspark_query'] = ''\n",
    "    \n",
    "    for i in range(len(avaliacao)):\n",
    "        query_text = avaliacao.loc[i, 'sql']\n",
    "        tipo_consulta = classify_product(query_text, keywords_mapping)\n",
    "        pyspark_query = convert_sql_to_pyspark(query_text)\n",
    "        \n",
    "        if tipo_consulta:\n",
    "            avaliacao.at[i, 'tipo'] = tipo_consulta\n",
    "        else:\n",
    "            avaliacao.at[i, 'tipo'] = 'not_identified'\n",
    "          \n",
    "        if pyspark_query:\n",
    "            avaliacao.at[i, 'pyspark_query'] = pyspark_query\n",
    "        else:\n",
    "            avaliacao.at[i, 'pyspark_query'] = 'not_identified'\n",
    "          \n",
    "    print(avaliacao[['sql', 'tipo', 'pyspark_query']])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
